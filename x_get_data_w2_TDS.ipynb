{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "x_get_data_w2_TDS.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP9b/Yb9Q1A/nEWomZkkT8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shailesh0209/x_tools_in_ds_dipoma-iitm/blob/main/x_get_data_w2_TDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L2.2: Get the data-Nominatim Open Street Maps"
      ],
      "metadata": {
        "id": "NwaFIOpj-RWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping using Geocoding API of Open Street Maps(OSM)\n",
        "We would be using the Nominatim API to scrape geocoding imformation of any open ended address text using Python"
      ],
      "metadata": {
        "id": "vALaMfUqA6sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no need to install these if using Google Colab\n",
        "!pip install geopandas\n",
        "!pip install geopy"
      ],
      "metadata": {
        "id": "ZudQeA60_gM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nominatim api\n",
        "from geopy.geocoders import Nominatim"
      ],
      "metadata": {
        "id": "OqJfQRvb_gJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate nominatim geocoder\n",
        "locator = Nominatim ()\n",
        "# type any address text\n",
        "location = locator.geocode(\"F-194, kankarbagh, Patna,Bihar, India\")\n"
      ],
      "metadata": {
        "id": "HggGLzirAEKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print latitude and longitude of the address \n",
        "print(\"Latitude={}, Longitude={}\".format(location.latitude, location.longitude))"
      ],
      "metadata": {
        "id": "BtT0t8xyAEH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the API output has multiple other details as a json like altitude, latitude\n",
        "# longitude, correct raw address, etc.\n",
        "# printing all the information\n",
        "\n",
        "location.raw, location.point, location.longitude, location.latitude, location.altitude, location.address"
      ],
      "metadata": {
        "id": "FScdhBsLTI46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# typing another address \n",
        "location2 = locator.geocode(\"IIT Madras\")"
      ],
      "metadata": {
        "id": "vXMF4ctVUD50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location2.raw, location2.point, location2.longitude, location2.latitude, location2.altitude, location2.address"
      ],
      "metadata": {
        "id": "ekOWSIV7Ufp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2.3 Get the data BBC Weather location service\n"
      ],
      "metadata": {
        "id": "_NWBitIz9t2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n0Uibg6iYWzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A tutorial to scrape the location ID of any city in BBC Weather\n",
        "\n",
        "Thsi code snippet takes city name as input and it hits the BBC Weather API with a request for location ID. This location ID is used as input in the next part of the code to scrape weather forecast for the city using this location ID.\n",
        "\n",
        "Web scraping might not be legal always. It is a good idea to check the terms of the website you plan to scrape before proceeding. Also, if your code requests a url from a server multiple times, it is a good practice to either cache your requests, o insert a timed delay between consecutive requests."
      ],
      "metadata": {
        "id": "fMGzX4t5YShT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import requests  # to get the webpage\n",
        "import json      # to convert API  to json format\n",
        "\n",
        "from urllib.parse import urlencode\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re        # regular expressio operators\n"
      ],
      "metadata": {
        "id": "bpMA_ZngYQrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_city = \"New York\"\n",
        "location_url = 'https://locator-service.api.bcci.co.uk/locations?' + urlencode({\n",
        "                'api_key': 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv',\n",
        "                's': test_city,\n",
        "                'stack': 'aws',\n",
        "                'locale': 'en',\n",
        "                'filter': 'international',\n",
        "                'place-types': 'settlement,airport, district',\n",
        "                'order': 'importance',\n",
        "                'a': 'true',\n",
        "                'format': 'json'\n",
        "                })\n",
        "location_url"
      ],
      "metadata": {
        "id": "6lm4KalkYQoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = requests.get(location_url, verify=False).text\n",
        "result"
      ],
      "metadata": {
        "id": "-eFa2f5VYQlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print locationid\n",
        "result['response']['results']['results'][0]['id']"
      ],
      "metadata": {
        "id": "NbDF3sBNYQiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating a function to output location id by taking any city name as input."
      ],
      "metadata": {
        "id": "uzQOl9xvjlwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getlocid(city):\n",
        "    city = city.lower() # convert city name to lowercase to standardize format\n",
        "    # Convert into an API call using URL encoding\n",
        "    location_url = 'https://locator-services.api.bcci.co.uk/locations?' + urlencode({\n",
        "        'api_key': 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv',\n",
        "        's': city,\n",
        "        'stack': 'aws',\n",
        "        'locale': 'en',\n",
        "        'filter': 'international',\n",
        "        'place-types': 'settlement, airport,district',\n",
        "        'order': 'true',\n",
        "        'format': 'json'\n",
        "    })\n",
        "    result = requests.get(location_url).json()\n",
        "    locid = result['response']['results']['results'][0]['id']\n",
        "    return locid"
      ],
      "metadata": {
        "id": "HdFs7V6VYQfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getlocid('Toronto')"
      ],
      "metadata": {
        "id": "h5RzUsfflIOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L2.4 Get the data-Scraping with Excel"
      ],
      "metadata": {
        "id": "_3Wqe2SU-RSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Paf1MNlemMy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U0_3Rv54mMtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iEvAwvP1mMp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AxepI2MWmMgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L2.5 Get the data-Scraping with Python"
      ],
      "metadata": {
        "id": "R3qcNzeb-RPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEB Scrapping IMDB\n",
        "\n",
        "In this exercise we'll look at scraping data from IMDB. Our goal is to convert the top 250 list of movies in IMDB intoa tabular form using Python. This data can then be used for further analysis."
      ],
      "metadata": {
        "id": "k_61RVfNGFzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Import Necessary Libraries"
      ],
      "metadata": {
        "id": "zgyG6eNwtbYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests # to access website\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DZIi0TVqGFSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Load the webpage"
      ],
      "metadata": {
        "id": "v9DbE6xRugGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get(\"https://www.imdb.com/chart/top/\")\n",
        "\n",
        "# Convert to a beautiful soup object\n",
        "soup = bs(r.content)\n",
        "\n",
        "# Print out HTML\n",
        "Contents = soup.prettify()"
      ],
      "metadata": {
        "id": "_7YhBOD_GByR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Creating empty list"
      ],
      "metadata": {
        "id": "nntWLGRDvAdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_title = []\n",
        "movie_year = []\n",
        "movie_rating = []"
      ],
      "metadata": {
        "id": "KKRSzVJ2GBuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Extract HTML tag contents"
      ],
      "metadata": {
        "id": "mjgkNUWZwpDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_table = soup.find(class_=\"chart full-width\")"
      ],
      "metadata": {
        "id": "-dJw5tUkGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_titlecolumn= imdb_table.find_all(class_=\"titleColumn\")"
      ],
      "metadata": {
        "id": "ORdSncZQ0uP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_ratingscolumn = imdb_table.find_all(class_=\"ratingColumn imdbRating\")"
      ],
      "metadata": {
        "id": "FcwRW2aM08aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in movie_titlecolumn:\n",
        "    title = row.a.text # tag content extraction\n",
        "    movie_title.append(title)\n",
        "movie_title"
      ],
      "metadata": {
        "id": "F1r8Qg7p1LnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in movie_titlecolumn:\n",
        "    year = row.span.text # tag content extraction # gives text contain inside span tag\n",
        "    movie_year.append(year)\n",
        "movie_year"
      ],
      "metadata": {
        "id": "bAqmCtcP1dYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in movie_ratingscolumn:\n",
        "    rating = row.strong.text # tag content extraction\n",
        "    movie_rating.append(rating)\n",
        "movie_rating"
      ],
      "metadata": {
        "id": "quuCXsSF3PYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Create DataFrame"
      ],
      "metadata": {
        "id": "zguvRjTO3o1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_df = pd.DataFrame({'Movie Title': movie_title, 'Year of Release': movie_year, 'IMDB Rating': movie_rating})\n",
        "movie_df"
      ],
      "metadata": {
        "id": "dQbc_gxh3oR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_df.head()"
      ],
      "metadata": {
        "id": "SuBFx3Wn6p7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tAvfrTd67wka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fVOW7rUr7wg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L2.6: Get the data-Wikimedia"
      ],
      "metadata": {
        "id": "pNG1pf2j-y9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a self-explanaory short tutorial on using the wikipedia library to extract information from wikipedia."
      ],
      "metadata": {
        "id": "gycJ5eeA71mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "import wikipedia as wk"
      ],
      "metadata": {
        "id": "FSummhTl7x2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wk.search(\"Isaac Newton\"))"
      ],
      "metadata": {
        "id": "9f-TauOw7xw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wk.search(\"IIT Madras\", results=2))"
      ],
      "metadata": {
        "id": "8ebHjEiJ7xt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wk.summary(\"Isaac Newton\"))"
      ],
      "metadata": {
        "id": "B66Xw1Mw7xpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wk.summary(\"IIT Madras\", sentences=2))"
      ],
      "metadata": {
        "id": "-dg1F_ro7xmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_page = wk.page(\"IIT Madras\")\n",
        "print(full_page.content)\n"
      ],
      "metadata": {
        "id": "9MdpPyWA7xh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_page.url)"
      ],
      "metadata": {
        "id": "NP71dfj37xee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_page.references)"
      ],
      "metadata": {
        "id": "mrBB-xyR7xY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_page.images)"
      ],
      "metadata": {
        "id": "RMGDfj6c7xU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_page.images[0])"
      ],
      "metadata": {
        "id": "WWupNVFs7xQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract html code of wikipedia page based on any search text\n",
        "html = wk.page(\"IIT Madras\").html().encode(\"UTF-8\")\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_html(html)[6]\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "t5YcmN57DBmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CLI4GwdAEl4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fwC-65G2Elzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "paM6MTqqElv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "38JqHhLyElsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2.7:Get the data-Scrape BBC weather with Python"
      ],
      "metadata": {
        "id": "GZH4yeek--Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A tutorial to scrape the web.\n",
        "\n",
        "This example scrapes the BBC weather for any specific city, and collects weather forecast for the next 14 days and saves it as a csv file.\n",
        "\n",
        "Web scraping might not be legal always. It is a good idea to check the terms of the website you plan to scrape before proceeding. Also, if your code requests a url from a server ultiple times, it is a good practice to either cache your requests, or insert a timed delay between consecutive requests."
      ],
      "metadata": {
        "id": "Vbx4d5X2YsgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json   # to convert API to json format\n",
        "\n",
        "from urllib.parse import urlencode\n",
        "\n",
        "import requests # to get the webpage \n",
        "from bs4 import BeautifulSoup # to parse the webpage \n",
        "\n",
        "import pandas as pd\n",
        "import re # regular expression operators\n",
        "\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "NJI7HX9JEnQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now GET the webpage of interest, from the server "
      ],
      "metadata": {
        "id": "e9t-BVcTbhRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "required_city = \"Mumbai\"\n",
        "location_url = 'https://loator-service.api.bbci.co.uk/locations?' + urlencode({\n",
        "    'api_key': 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv',\n",
        "    's': required_city,\n",
        "    'stack': 'aws',\n",
        "    'locale': 'en',\n",
        "    'filter': 'international',\n",
        "    'pace-types': 'settlement,airport,district',\n",
        "    'order': 'importance',\n",
        "    'a': 'true',\n",
        "    'format': 'json'\n",
        "})\n",
        "location_url"
      ],
      "metadata": {
        "id": "ukH3MrD4EnMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = requests.get(location_url, verify=False).json()\n",
        "result"
      ],
      "metadata": {
        "id": "BoZn39vNEnJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" url = 'https//www.bbc.com/weather/1275339' # url to BBC weather, \n",
        "corresponding to a specific city (Mumbai, in this example\"\"\"\n",
        "\n",
        "url = 'https://www.bbc.com/weather/'+result['response']['results']['results'][0]['id']\n",
        "response = requests.get(url, verify=False)"
      ],
      "metadata": {
        "id": "rKBlpMkEEnFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we initiate an instance fo BeautifulSoup."
      ],
      "metadata": {
        "id": "YkmjGXdCeE1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "YT-ZpQLlEnBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information we want (daily high and low temp., and daily weather summary), are in specific blocks on the webpage. We need to find the block type, type of identifier, and the identifier name (all these can be figured out by right clicking on the webpage and selecting 'Inspect' on the Chrome browser; similar modus operand for the browsers)"
      ],
      "metadata": {
        "id": "dXeX-qA3eWFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_high_values = soup.find_all('span', \n",
        "                    attrs={'class': \n",
        "                    'wr-day-temperature_high-value'})\n",
        "# block-type: span; identifier type: class; and \n",
        "#class name: wr-day-temperature_high-value\n",
        "daily_high_values"
      ],
      "metadata": {
        "id": "8FEjwqCNEm-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_low_values = soup.find_all('span', \n",
        "                attrs={'class': 'wr-day-temperature_low-value'})\n",
        "daily_low-values"
      ],
      "metadata": {
        "id": "EhVYG9XJEm6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_summary = soup.find('div',\n",
        "            attrs={'class': 'wr-day-summary'})\n",
        "daily_sumary"
      ],
      "metadata": {
        "id": "RWQgpEIyg7Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_summary.text"
      ],
      "metadata": {
        "id": "dxUy3RMrg6_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`General book keeping`:\n",
        "With the code snippet in the cell above, we get forecast data for 14 days, including today. We will now post process the data to first extract the required information/text and discard all the html wrapper code, then combine all variables into one common list, and finally convert it into a pandas dataFrame."
      ],
      "metadata": {
        "id": "GqiV2m18MEEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_high_values[0].text.strip()\n"
      ],
      "metadata": {
        "id": "0O6k5Gcug6x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_high_values[5].text.strip()"
      ],
      "metadata": {
        "id": "8edl2PDUg6uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_high_values[0].text.strip().split()[0]"
      ],
      "metadata": {
        "id": "jxgFnS8Tg6sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_high_values_list = [daily_high_values[i].text.strip().split()[0] for i in range(len(daily_high_values))]\n",
        "daily_high_values_list"
      ],
      "metadata": {
        "id": "V1pOH08tg6pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_low_values_list = [daily_low_values[i].text.strip().split()[0] for i in range(len(daily_low_values))]\n",
        "daily_low_values_list"
      ],
      "metadata": {
        "id": "KCyahuulg6m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_summary.text"
      ],
      "metadata": {
        "id": "z7Ou2o4eNrCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_summary_list = re.fidall('[a-zA-Z][^A-Z]*', daily_summary.text) # split the string on uppercase\n",
        "daily_summary_list"
      ],
      "metadata": {
        "id": "IOw-lCvINtZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datelist = pd.date_range(datetime.today(), periods=len(daily_high_values)).tolist()\n",
        "datelist"
      ],
      "metadata": {
        "id": "viN7meg0NtUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datelist = [datelist[i].date().strftime('%y-%m-%d') for i in range(len(datelist))]\n",
        "datelist"
      ],
      "metadata": {
        "id": "l-SdnPVDO5V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipped = zip(datelist, daily_high_values_list, daily_low_values_list, daily_summary_list)"
      ],
      "metadata": {
        "id": "NESmI2MyO5Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zipped), columns=['Date', 'High', 'Low', 'Summary'])"
      ],
      "metadata": {
        "id": "htpsH1GwO5Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "CmSkDGukO5I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the 'degree' character\n",
        "df.High = df.High.replace('\\°','',regex=True).astype(float)\n",
        "df.Low = df.Low.replace('\\°','',regex=True).astype(float)"
      ],
      "metadata": {
        "id": "O8qh4CqzO5GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "ScFTcPhEUw0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the name of the city for which data is gathered\n"
      ],
      "metadata": {
        "id": "HsV-NhXQUzNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# location = soup.find('div', attrs={'class':'wr-c-location'})\n",
        "location = soup.find('h1', attrs={'id':'wr-location-name-id'})\n",
        "location.text.split()"
      ],
      "metadata": {
        "id": "veNFiCJyUwq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a recording\n",
        "filename_csv = location.text.split()[0]+'.csv'\n",
        "df.to_csv(filename_csv,index = None)"
      ],
      "metadata": {
        "id": "A7M4zAvwUwnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_xlsx = location.text.split()[0]+'.xlsx'\n",
        "df.to_exvel(filename_xlsx)"
      ],
      "metadata": {
        "id": "qSwTBifCUwkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N1-0xJ0eUwhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FQMTps-vg6Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L2.8: Get the data-Scraping PDFs"
      ],
      "metadata": {
        "id": "CjiqhibQ_F2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mYIiCEhiEpCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UmQULxmQEo-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lhcGwuIJEo1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9_Gbfx9eEopn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TgFxAV4WEomR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xVr-fqoYEojy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2mLUjrjlEofy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pE4IiHXFEob1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GA2"
      ],
      "metadata": {
        "id": "orUfXEZC_Ma7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k3VEQpz_9_Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31LhgcS59SEJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}